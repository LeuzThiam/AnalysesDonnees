from __future__ import annotations

import os
import re
import logging
from math import isnan

import duckdb
import numpy as np
import pandas as pd
from django.conf import settings
from django.http import JsonResponse
from rest_framework.decorators import api_view, parser_classes, permission_classes
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny

# ‚úÖ Imports internes
from .duck import (
    load_to_duckdb,
    list_tables,
    profile_table,
    run_sql,
    auto_analyze,
)
from .services.guards import is_safe
from .services.runners import run_sql_safe
from .services.planner import build_sql_from_plan
from integrations.n8n import nl_to_sql as n8n_nl_to_sql, is_configured as n8n_is_configured
from integrations.n8n_analysis import analyze_result, is_configured as analysis_is_configured

from .services.pandas_runner import run_pandas_analysis

# ============================================================
# üîß Correction automatique du type de graphique (fallback)
# ============================================================

def auto_fix_chart_spec(question: str, chart_spec: dict, rows: list) -> dict:
    """Corrige ou d√©duit automatiquement le type de graphique selon la question."""
    q = question.lower().strip()
    if not isinstance(chart_spec, dict):
        chart_spec = {}

    # üß† D√©tection du type si manquant ou 'table'
    if not chart_spec.get("type") or chart_spec.get("type") == "table":
        if any(k in q for k in ["r√©partition", "proportion", "distribution", "part", "pourcentage", "par nationalit√©", "par pays"]):
            chart_spec["type"] = "pie"
        elif any(k in q for k in ["√©volution", "temps", "mois", "jour", "chronologique", "par semaine", "timeline"]):
            chart_spec["type"] = "line"
        elif any(k in q for k in ["classement", "top", "meilleur", "pire", "comparaison", "plus", "moins"]):
            chart_spec["type"] = "bar"
        elif any(k in q for k in ["corr√©lation", "relation", "vs", "entre"]):
            chart_spec["type"] = "scatter"
        elif any(k in q for k in ["zone", "aire", "cumul", "progression"]):
            chart_spec["type"] = "area"
        elif any(k in q for k in ["entonnoir", "funnel"]):
            chart_spec["type"] = "funnel"
        elif any(k in q for k in ["radial", "cercle concentrique", "progression circulaire"]):
            chart_spec["type"] = "radial_bar"
        elif any(k in q for k in ["treemap", "hi√©rarchie", "structure"]):
            chart_spec["type"] = "treemap"
        elif any(k in q for k in ["radar", "comp√©tences", "profil", "polygone"]):
            chart_spec["type"] = "radar"
        elif any(k in q for k in ["empil√©", "stacked"]):
            chart_spec["type"] = "stacked_bar"
        else:
            chart_spec["type"] = "table"  # par d√©faut, renvoyer table si rien n'est graphique


    # üîí S√©curisation des cl√©s x/y √† partir des donn√©es r√©elles
    if "x" not in chart_spec and rows and isinstance(rows[0], dict):
        chart_spec["x"] = list(rows[0].keys())[0]
    if "y" not in chart_spec and rows and isinstance(rows[0], dict):
        chart_spec["y"] = list(rows[0].keys())[1] if len(rows[0]) > 1 else list(rows[0].keys())[0]

    return chart_spec


logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# üß© Normalisation des noms
# ---------------------------------------------------------------------------

def _normalize_dataset_name(name: str) -> str:
    """Nettoie un nom de dataset pour qu‚Äôil soit compatible avec DuckDB."""
    return re.sub(r"[^A-Za-z0-9]+", "_", (name or "").strip()).strip("_").lower()


def _normalize_column_name(name: str) -> str:
    """Nettoie un nom de colonne pour l‚Äôusage SQL."""
    col = re.sub(r"\W+", "_", str(name).strip().lower())
    if re.match(r"^\d", col):
        col = f"col_{col}"
    return col


def _infer_left_var_from_read_csv(code: str) -> str | None:
    """D√©tecte la variable affect√©e au read_csv pour r√©injection DuckDB."""
    if not code:
        return None
    m = re.search(r"^\s*([A-Za-z_]\w*)\s*=\s*pd\.read_csv\(", code, flags=re.MULTILINE)
    return m.group(1) if m else None


def _inject_duckdb_preamble(code: str, table_name: str, prefer_var: str | None = None) -> str:
    """Injecte le chargement DuckDB dans le code Python g√©n√©r√© par le LLM."""
    left_var = _infer_left_var_from_read_csv(code) or prefer_var or "df"
    patched = re.sub(r"pd\.read_csv\([^)]*\)", left_var, code or "", flags=re.MULTILINE)

    preamble = f"""
import duckdb as _ddb
_con = _ddb.connect(r'{settings.DUCKDB_PATH}')
{left_var} = _con.execute('SELECT * FROM "{table_name}"').df()
"""
    return preamble.strip() + "\n\n" + patched.strip()


# ---------------------------------------------------------------------------
# ‚ôªÔ∏è Helpers g√©n√©raux de r√©ponse/validation
# ---------------------------------------------------------------------------


def _json_error(detail: str, *, status: int = 400, code: str | None = None, extra: dict | None = None) -> JsonResponse:
    payload = {"detail": detail}
    if code:
        payload["code"] = code
    if extra:
        payload.update(extra)
    return JsonResponse(payload, status=status)


def _dataset_exists(name: str) -> bool:
    if not name:
        return False
    try:
        tables = list_tables()
        return any(table.lower() == name.lower() for table in tables)
    except duckdb.Error:
        logger.exception("Impossible de v√©rifier l'existence du dataset %s", name)
        return False


def _safe_limit(raw_limit: str | int | None, *, default: int = 10, max_value: int = 1000) -> int:
    try:
        limit = int(raw_limit) if raw_limit is not None else default
    except (TypeError, ValueError):
        raise ValueError("Param√®tre 'limit' invalide")
    return max(1, min(limit, max_value))


def _to_serializable(value):
    if value is None:
        return None
    if isinstance(value, (np.generic,)):
        return value.item()
    if isinstance(value, (pd.Timestamp, pd.Timedelta)):
        return value.isoformat()
    if isinstance(value, float) and isnan(value):
        return None
    return value


def _frame_to_records(df: pd.DataFrame, *, limit: int | None = None) -> list[dict]:
    sample = df.head(limit) if limit is not None else df
    if sample.empty:
        return []
    serialised = sample.where(pd.notnull(sample), None)
    return [
        {k: _to_serializable(v) for k, v in row.items()}
        for row in serialised.to_dict(orient="records")
    ]


# ---------------------------------------------------------------------------
# üìä Profilage automatique
# ---------------------------------------------------------------------------

_DATE_SYNONYMS = ("date", "jour", "day", "time", "timestamp", "datetime", "created", "due")
_NUM_SYNONYMS = ("cases", "cas", "total_cases", "value", "amount", "sum", "count", "nb", "y")
_ID_LIKE = {"id", "task id", "task_id"}

def _duck_profile(dataset: str) -> dict:
    return profile_table(dataset, limit=50) or {}


def _is_num_dtype(dt: str) -> bool:
    s = str(dt).lower()
    return any(k in s for k in ("int", "float", "double", "decimal", "numeric"))


def _is_dt_dtype(dt: str) -> bool:
    s = str(dt).lower()
    return any(k in s for k in ("date", "time", "timestamp", "datetime"))


def _infer_columns(dataset: str):
    """Retourne (date_col, num_col, cat_col) √† partir du profil."""
    info = _duck_profile(dataset)
    cols = info.get("columns", [])

    # üîπ Colonne de date
    date_col = next((c["name"] for c in cols if _is_dt_dtype(c["dtype"])), None)
    if not date_col:
        for p in _DATE_SYNONYMS:
            for c in cols:
                if p in str(c["name"]).lower().replace(" ", "_"):
                    date_col = c["name"]
                    break
            if date_col:
                break

    # üîπ Colonne num√©rique
    num_col = next(
        (c["name"] for c in cols if _is_num_dtype(c["dtype"]) and str(c["name"]).lower() not in _ID_LIKE),
        None,
    )
    if not num_col:
        for p in _NUM_SYNONYMS:
            for c in cols:
                if p in str(c["name"]).lower().replace(" ", "_"):
                    num_col = c["name"]
                    break
            if num_col:
                break

    # üîπ Colonne cat√©gorielle
    cat_col = next(
        (c["name"] for c in cols if not _is_num_dtype(c["dtype"]) and not _is_dt_dtype(c["dtype"])),
        None,
    )

    return date_col, num_col, cat_col


# ---------------------------------------------------------------------------
# üß† G√©n√©ration SQL automatique selon le type de graphique
# ---------------------------------------------------------------------------

def _synth_sql_from_spec(dataset: str, spec: dict) -> tuple[str | None, dict]:
    """Retourne (sql, spec_normalis√©e). G√®re aussi les histogrammes automatiques."""
    if not isinstance(spec, dict):
        return None, spec or {}

    typ = (spec.get("type") or "").lower()
    spec_norm = dict(spec)

    date_col, num_col, cat_col = _infer_columns(dataset)

    def q(col: str) -> str:
        return f'"{col}"'

    # üîπ Histogramme simple (ex : distribution des valeurs)
    if typ == "histogram":
        x = spec.get("x") or num_col
        if not x:
            return None, spec_norm
        spec_norm.setdefault("bins", 20)
        return f'SELECT {q(x)} FROM "{dataset}" WHERE {q(x)} IS NOT NULL', spec_norm

    # üîπ Graphique en barres
    if typ in ("bar", "bar_horizontal"):
        x = spec.get("x") or cat_col
        y = spec.get("y") or num_col
        if not (x and y):
            return None, spec_norm
        sql = f'''
SELECT {q(x)} AS label, SUM(CAST({q(y)} AS DOUBLE)) AS value
FROM "{dataset}"
WHERE {q(x)} IS NOT NULL AND {q(y)} IS NOT NULL
GROUP BY 1
ORDER BY 2 DESC;
'''
        return sql.strip(), spec_norm

    # üîπ S√©rie temporelle
    if typ in ("line", "timeseries"):
        x = spec.get("x") or date_col
        y = spec.get("y") or num_col
        if not (x and y):
            return None, spec_norm
        sql = f'''
SELECT CAST({q(x)} AS TIMESTAMP) AS dt, SUM(CAST({q(y)} AS DOUBLE)) AS value
FROM "{dataset}"
WHERE {q(x)} IS NOT NULL AND {q(y)} IS NOT NULL
GROUP BY 1
ORDER BY 1;
'''
        return sql.strip(), spec_norm

    # üîπ Graphique circulaire
    if typ == "pie":
        label = spec.get("label") or cat_col
        value = spec.get("value") or num_col
        if not (label and value):
            return None, spec_norm
        sql = f'''
SELECT {q(label)} AS name, SUM(CAST({q(value)} AS DOUBLE)) AS value
FROM "{dataset}"
WHERE {q(label)} IS NOT NULL AND {q(value)} IS NOT NULL
GROUP BY 1
ORDER BY 2 DESC;
'''
        return sql.strip(), spec_norm

    # üîπ Table simple
    if typ == "table":
        return f'SELECT * FROM "{dataset}" LIMIT 1000;', spec_norm

    # üß† Nouvelle logique : d√©tection automatique d‚Äôhistogrammes
    # Si le LLM ne pr√©cise pas le type, on tente de deviner √† partir du SQL
    if "GROUP BY" in spec.get("sql", "").upper() or "COUNT(" in spec.get("sql", "").upper():
        spec_norm.setdefault("type", "histogram")
        spec_norm.setdefault("x", cat_col or num_col or "x")
        spec_norm.setdefault("y", "count")

    return None, spec_norm


# ---------------------------------------------------------------------------
# üåê Endpoints API
# ---------------------------------------------------------------------------

@api_view(["POST"])
@parser_classes([MultiPartParser, FormParser])
@permission_classes([AllowAny])
def upload_dataset(request):
    """Upload et chargement dans DuckDB."""
    try:
        upfile = request.FILES.get("file")
        if not upfile:
            return _json_error("Champ 'file' requis.")

        dataset = _normalize_dataset_name(request.data.get("dataset") or os.path.splitext(upfile.name)[0])
        ext = (upfile.name or "").lower().rsplit(".", 1)[-1]

        if ext in ("csv", "xlsx", "xls", "json", "parquet"):
            info = load_to_duckdb(upfile, dataset, file_type=ext if ext != "xls" else "excel")
            return JsonResponse({"ok": True, "table": dataset, **info}, status=201)

        return _json_error("Format non support√© (CSV, XLSX, JSON, Parquet).")

    except Exception as e:
        logger.exception("upload_dataset: erreur inattendue")
        return _json_error(f"Echec import: {e}", status=500)


@api_view(["GET"])
@permission_classes([AllowAny])
def datasets_list(request):
    try:
        return JsonResponse({"tables": list_tables()})
    except Exception as e:
        logger.exception("datasets_list: erreur inattendue")
        return _json_error(str(e), status=500)


@api_view(["GET"])
@permission_classes([AllowAny])
def datasets_preview(request, table: str):
    try:
        try:
            limit = _safe_limit(request.GET.get("limit"), default=10)
        except ValueError as exc:
            return _json_error(str(exc))

        if not _dataset_exists(table):
            return _json_error(f"Dataset '{table}' introuvable.", status=404, code="dataset_not_found")

        info = profile_table(table, limit=limit)
        return JsonResponse({"table": table, **info})
    except Exception as e:
        logger.exception("datasets_preview: erreur inattendue")
        return _json_error(str(e), status=500)


@api_view(["GET"])
@permission_classes([AllowAny])
def datasets_insights(request, table: str):
    """Fournit des statistiques descriptives et corr√©lations cl√©s pour un dataset."""
    try:
        if not _dataset_exists(table):
            return _json_error(f"Dataset '{table}' introuvable.", status=404, code="dataset_not_found")

        try:
            sample_limit = _safe_limit(request.GET.get("sample"), default=500, max_value=5000)
        except ValueError as exc:
            return _json_error(str(exc))

        try:
            df = duckdb.sql(f'SELECT * FROM "{table}" LIMIT {sample_limit}').to_df()
        except duckdb.Error as exc:
            logger.warning("datasets_insights: lecture du dataset %s impossible: %s", table, exc)
            return _json_error(f"Impossible de lire le dataset '{table}'.", status=500)

        if df.empty:
            return JsonResponse(
                {
                    "table": table,
                    "rows_sampled": 0,
                    "fields": {},
                    "summary": [],
                    "missing_values": {},
                    "correlations": [],
                }
            )

        try:
            summary_df = df.describe(include="all", datetime_is_numeric=True)
        except TypeError:
            summary_df = df.describe(include="all")
        summary_df = summary_df.transpose().reset_index()
        summary_df = summary_df.rename(columns={"index": "field"})
        summary = [
            {k: _to_serializable(v) for k, v in row.items()}
            for row in summary_df.where(pd.notnull(summary_df), None).to_dict(orient="records")
        ]

        missing_values = {
            column: _to_serializable(int(count))
            for column, count in df.isna().sum().to_dict().items()
        }

        numeric_df = df.select_dtypes(include=[np.number])
        correlations: list[dict] = []
        if not numeric_df.empty and numeric_df.shape[1] > 1:
            corr_matrix = numeric_df.corr()
            for i, col_i in enumerate(corr_matrix.columns):
                for j in range(i + 1, len(corr_matrix.columns)):
                    value = corr_matrix.iloc[i, j]
                    correlations.append(
                        {
                            "feature_1": col_i,
                            "feature_2": corr_matrix.columns[j],
                            "correlation": _to_serializable(value),
                        }
                    )
            correlations.sort(
                key=lambda item: abs(item["correlation"]) if item["correlation"] is not None else -1,
                reverse=True,
            )

        response = {
            "table": table,
            "rows_sampled": len(df),
            "fields": {col: str(dtype) for col, dtype in df.dtypes.items()},
            "summary": summary,
            "missing_values": missing_values,
            "correlations": correlations[:50],
            "sample": _frame_to_records(df, limit=50),
        }

        return JsonResponse(response)
    except Exception as exc:
        logger.exception("datasets_insights: erreur inattendue")
        return _json_error(f"Erreur interne: {exc}", status=500)


@api_view(["POST"])
@permission_classes([AllowAny])
def query_sql(request):
    """Ex√©cute une requ√™te SQL brute (avec v√©rification de s√©curit√©)."""
    try:
        sql = (request.data.get("sql") or "").strip()
        if not sql:
            return _json_error("Champ 'sql' requis.")

        if not is_safe(sql):
            return _json_error("Requ√™te SQL non autoris√©e.")

        rows = run_sql_safe(sql)
        return JsonResponse({"rows": rows})
    except duckdb.CatalogException as exc:
        logger.warning("query_sql: dataset introuvable: %s", exc)
        return _json_error("Dataset introuvable dans la requ√™te.", status=404, code="dataset_not_found")
    except Exception as e:
        logger.exception("query_sql: erreur inattendue")
        return _json_error(str(e), status=500)


# ---------------------------------------------------------------------------
# üß† Requ√™tes NL ‚Üí SQL via LLM
# ---------------------------------------------------------------------------

def get_schema(dataset: str) -> str:
    """R√©cup√®re les noms et types de colonnes DuckDB (pour contextualiser le LLM)."""
    if not dataset:
        return "Aucun dataset sp√©cifi√©"

    try:
        df = duckdb.query(f"DESCRIBE {dataset};").to_df()
        if df.empty:
            return "Aucune colonne d√©tect√©e"
        cols = [f"{row['column_name']} ({row['column_type']})" for _, row in df.iterrows()]
        return ", ".join(cols)
    except Exception as e:
        logger.warning(f"Impossible de lire le sch√©ma pour {dataset}: {e}")
        return "Sch√©ma non disponible"
    

import numpy as np

def auto_analyze_result(rows: list[dict], chart_spec: dict, question: str) -> str:
    """
    G√©n√®re une interpr√©tation automatique simple selon les r√©sultats.
    """
    if not rows or not isinstance(rows, list) or len(rows) == 0:
        return "Aucune donn√©e √† analyser."

    # On essaie d‚Äôidentifier les colonnes num√©riques et cat√©gorielles
    first_row = rows[0]
    keys = list(first_row.keys())
    if len(keys) < 2:
        return "R√©sultat trop simple pour une analyse automatique."

    x_key = chart_spec.get("x", keys[0])
    y_key = chart_spec.get("y", keys[1])
    try:
        y_values = [float(r[y_key]) for r in rows if r.get(y_key) is not None]
    except Exception:
        return "Impossible d‚Äôinterpr√©ter les valeurs num√©riques."

    # Statistiques de base
    n = len(y_values)
    mean_val = np.mean(y_values)
    median_val = np.median(y_values)
    min_val = np.min(y_values)
    max_val = np.max(y_values)
    amplitude = max_val - min_val

    # G√©n√©ration d‚Äôune analyse textuelle
    analysis = []

    # Type d√©tect√©
    typ = (chart_spec.get("type") or "").lower()

    if typ in ["line", "timeseries", "area"]:
        analysis.append(f"Les donn√©es pr√©sentent une √©volution sur {n} points.")
        if y_values[-1] > y_values[0]:
            analysis.append("La tendance g√©n√©rale est √† la hausse üìà.")
        elif y_values[-1] < y_values[0]:
            analysis.append("La tendance g√©n√©rale est √† la baisse üìâ.")
        else:
            analysis.append("La tendance reste stable sur la p√©riode.")
    elif typ in ["bar", "pie", "histogram", "stacked_bar"]:
        max_row = max(rows, key=lambda r: r[y_key])
        min_row = min(rows, key=lambda r: r[y_key])
        analysis.append(f"La cat√©gorie '{max_row[x_key]}' a la valeur la plus √©lev√©e ({max_row[y_key]:.2f}).")
        analysis.append(f"La cat√©gorie '{min_row[x_key]}' est la plus faible ({min_row[y_key]:.2f}).")
        if amplitude / mean_val > 0.5:
            analysis.append("La variation entre cat√©gories est importante.")
        else:
            analysis.append("Les valeurs sont relativement homog√®nes entre cat√©gories.")
    else:
        analysis.append("Les r√©sultats sont affich√©s sous forme tabulaire. Consultez les valeurs cl√©s ci-dessus.")

    analysis.append(f"Valeur moyenne : {mean_val:.2f}, m√©diane : {median_val:.2f}, min : {min_val:.2f}, max : {max_val:.2f}.")
    return " ".join(analysis)





@api_view(["POST"])
@permission_classes([AllowAny])
def query_nl(request):
    """
    NL ‚Üí (n8n) ‚Üí SQL/plan ‚Üí ex√©cution s√©curis√©e ‚Üí (optionnel) analyse experte n8n
    avec fallback local si n8n indisponible.
    """
    try:
        data = request.data or {}
        question = (data.get("question") or "").strip()
        dataset = _normalize_dataset_name(data.get("dataset"))

        if not question or not dataset:
            return _json_error("Champs 'question' et 'dataset' requis.")

        if not _dataset_exists(dataset):
            return _json_error(f"Dataset '{dataset}' introuvable.", status=404, code="dataset_not_found")

        # 1) Sch√©ma pour contextualiser
        schema = get_schema(dataset)
        extra = {k: v for k, v in data.items() if k not in {"question", "dataset"}}
        extra.update({"schema": schema})

        # 2) NL‚ÜíSQL via n8n (si dispo)
        payload = {}
        if n8n_is_configured():
            try:
                payload = n8n_nl_to_sql(question, dataset, extra=extra)
            except Exception as e:
                logger.warning(f"Erreur n8n (NL‚ÜíSQL): {e}")

        # 3) Cas code Python g√©n√©r√©
        if payload.get("code_python"):
            code = _inject_duckdb_preamble(payload["code_python"], dataset, prefer_var=dataset)
            result = run_pandas_analysis(code)
            rows = result.get("rows", [])
            chart_spec = payload.get("chart_spec", {"type": "custom"})
            # Analyse experte (optionnelle) + fallback local
            analysis_text = ""
            if analysis_is_configured():
                try:
                    n8n_out = analyze_result(question, rows, chart_spec)
                    analysis_text = n8n_out.get("summary") or n8n_out.get("text") or ""
                except Exception as e:
                    logger.warning(f"Analyse n8n √©chou√©e: {e}")
                    analysis_text = auto_analyze_result(rows, chart_spec, question)
            else:
                analysis_text = auto_analyze_result(rows, chart_spec, question)

            return JsonResponse({
                "rows": rows,
                "chart_spec": chart_spec,
                "summary": (payload.get("summary") or "Analyse automatique g√©n√©r√©e.")
                           + (f"\n\nüí° Analyse experte (n8n) : {analysis_text}" if analysis_text else ""),
                "sql": payload.get("sql"),
                "schema": schema,
            })

        # 4) Cas SQL g√©n√©r√© (ou synth√®se depuis chart_spec / plan)
        chart_spec = payload.get("chart_spec", {}) or {}
        sql = (payload.get("sql") or "").strip()

        if not sql and chart_spec:
            sql, chart_spec = _synth_sql_from_spec(dataset, chart_spec)

        if not sql:
            date_col, val_col, cat_col = _infer_columns(dataset)
            plan = {
                "intent": data.get("intent", "timeseries_total"),
                "dataset": dataset,
                "date_col": date_col,
                "amount_col": val_col,
                "category_col": cat_col,
                "limit": int(data.get("limit", 1000)),
            }
            sql = build_sql_from_plan(plan)
            chart_spec = {"type": "table"}

        # 5) S√©curit√© puis ex√©cution
        if not sql or not is_safe(sql):
            return _json_error("SQL g√©n√©r√© invalide ou non autoris√©.")

        try:
            rows = run_sql_safe(sql)
        except Exception as e:
            logger.error(f"Erreur ex√©cution SQL ({dataset}): {e}")
            return _json_error(f"Ex√©cution SQL √©chou√©e: {e}")

        # 6) Fix chart + analyse locale / n8n
        chart_spec = auto_fix_chart_spec(question, chart_spec, rows)

        analysis_text = ""
        if analysis_is_configured():
            try:
                n8n_out = analyze_result(question, rows, chart_spec)
                analysis_text = n8n_out.get("summary") or n8n_out.get("text") or ""
            except Exception as e:
                logger.warning(f"Analyse n8n √©chou√©e: {e}")
                analysis_text = auto_analyze_result(rows, chart_spec, question)
        else:
            analysis_text = auto_analyze_result(rows, chart_spec, question)

        combined_summary = payload.get("summary") or "Analyse automatique g√©n√©r√©e."
        if analysis_text:
            combined_summary += f"\n\nüí° Analyse experte : {analysis_text}"

        return JsonResponse({
            "rows": rows,
            "chart_spec": chart_spec,
            "summary": combined_summary,
            "sql": sql,
            "schema": schema,
        })



    except Exception as e:
        logger.exception("query_nl: erreur inattendue")
        return _json_error(f"Erreur interne: {e}", status=500)
